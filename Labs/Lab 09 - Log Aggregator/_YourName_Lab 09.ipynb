{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [],
   "source": [
    import logging
import random
import time
import os

# different log files to write to
log_files = [
    "sqldb.log",
    "ui.log",
    "frontend.js.log",
    "backend.js.log",
    "frontend.flask.log",
    "backend.flask.log"
]

# messages based on severity
messages = {
    "INFO": ["Request completed", "Heartbeat OK", "User logged in", "Data synced"],
    "WARNING": ["High memory usage", "Slow API response", "Deprecated API call"],
    "ERROR": ["Unhandled exception occurred", "Database connection lost", "Timeout on request"],
    "CRITICAL": ["Disk failure detected", "Database corrupted", "Memory leak found"]
}

# function to setup a logger for each file
def setup_logger(logfile):
    logger = logging.getLogger(logfile)
    logger.setLevel(logging.DEBUG)
    handler = logging.FileHandler(logfile)
    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s', "%Y-%m-%d %H:%M:%S")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return logger

def generate_logs():
    if not os.path.exists("Logs"):
        os.makedirs("Logs")
        
    # setting up all the loggers
    loggers = {}
    for file in log_files:
        loggers[file] = setup_logger(os.path.join("Logs", file))
    
    # writing random logs
    for i in range(120):
        file = random.choice(log_files)
        level = random.choice(list(messages.keys()))
        message = random.choice(messages[level])
        logger = loggers[file]

        if level == "INFO":
            logger.info(message)
        elif level == "WARNING":
            logger.warning(message)
        elif level == "ERROR":
            logger.error(message)
        else:
            logger.critical(message)

        time.sleep(0.05) # not too fast

if __name__ == "__main__":
    generate_logs()

   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    import os
import re
import json

# read lines from a file
def read_log_file(filepath):
    try:
        with open(filepath, 'r') as f:
            return f.readlines()
    except Exception as e:
        print(f"Failed to read {filepath}: {e}")
        return []

# pull apart a log line
def parse_line(line):
    pattern = r"^(.*?)\s\|\s([\w\.]+)\s\|\s(\w+)\s\|\s(.*)$"
    match = re.match(pattern, line)
    if match:
        return match.groups()
    else:
        return None

# count number of each log level + message
def count_logs(lines):
    counts = {}
    for line in lines:
        parsed = parse_line(line)
        if parsed:
            time_stamp, logger_name, level, msg = parsed
            if level not in counts:
                counts[level] = {}
            if msg not in counts[level]:
                counts[level][msg] = 1
            else:
                counts[level][msg] += 1
    return counts

def main():
    all_lines = []
    logs_dir = "Logs"

    # grab all the logs
    for file in os.listdir(logs_dir):
        if file.endswith(".log"):
            path = os.path.join(logs_dir, file)
            lines = read_log_file(path)
            all_lines.extend(lines)
    
    summary = count_logs(all_lines)

    # output to JSON
    with open("log_summary.json", "w") as f:
        json.dump(summary, f, indent=4)
    
    print("Log summary written to log_summary.json!")

if __name__ == "__main__":
    main()

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here \n",
    "# don't forget to upload it with your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3a: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4429f",
   "metadata": {},
   "outputs": [],
   "source": [
    
import threading
import json
import os
import time
import matplotlib.pyplot as plt

last_mod_time = None

# check for file changes
def check_file(filename):
    global last_mod_time
    if not os.path.exists(filename):
        return
    
    current_time = os.path.getmtime(filename)
    if last_mod_time is None or current_time != last_mod_time:
        last_mod_time = current_time
        with open(filename, 'r') as f:
            data = json.load(f)
            print_summary(data)
            draw_graph(data)

# print logs
def print_summary(data):
    print("\n=== Log Summary Update ===")
    for level, messages in data.items():
        total = sum(messages.values())
        print(f"{level}: {total} entries")
        if level == "CRITICAL":
            for msg, cnt in messages.items():
                print(f" - {msg}: {cnt}")
    print("==========================\n")

# draw a simple bar graph
def draw_graph(data):
    labels = []
    values = []

    for level in data:
        labels.append(level)
        values.append(sum(data[level].values()))
    
    plt.figure()
    plt.bar(labels, values)
    plt.title("Logs by Level")
    plt.xlabel("Level")
    plt.ylabel("Count")
    plt.show(block=False)
    plt.pause(2)
    plt.close()

# background watcher thread
def background_task():
    while True:
        check_file("log_summary.json")
        time.sleep(2)

def main():
    watch_thread = threading.Thread(target=background_task, daemon=True)
    watch_thread.start()

    # main loop to keep alive
    while True:
        print("Watching for changes...")
        time.sleep(5)

if __name__ == "__main__":
    main()

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26eb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample regex that parses a log file and extracts relevant information. \n",
    "# you will need to modify it. Review Lecture 11\n",
    "import re\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = r\"^(.*?)\\s\\|\\s(\\w+)\\s\\|\\s(\\w+)\\s\\|\\s(.*)$\"\n",
    "    match = re.match(pattern, line)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
